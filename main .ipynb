{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d202b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import requests\n",
    "from lxml.html import fromstring\n",
    "import json\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2062c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Combine multiple regex patterns for optimization\n",
    "    text = re.sub('<.*?>|ENDOFARTICLE.|\\\\d+|\\\\n| +', ' ', text)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_random_file(path):\n",
    "    \"\"\"Get a random file from a directory.\"\"\"\n",
    "    txt_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".txt\")]\n",
    "    return random.choice(txt_files) if txt_files else None\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Tokenize, remove stopwords and lemmatize.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc if token.text.strip()]\n",
    "    filtered = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 1]\n",
    "    return filtered\n",
    "\n",
    "def get_bigrams(words):\n",
    "    bigrams = ngrams(words, 2)\n",
    "    bigram_counts = Counter(bigrams).most_common(10)\n",
    "    return [' '.join(bigram) for bigram, count in bigram_counts]\n",
    "\n",
    "def pos_tagging(words):\n",
    "    \"\"\"Tag words with POS and return filtered nouns.\"\"\"\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    return [word for word, pos in pos_tags if pos == 'NN']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5600c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UMLSAuthentication:\n",
    "    \"\"\"Class for handling UMLS API authentication.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.uri = \"https://utslogin.nlm.nih.gov\"\n",
    "        self.auth_endpoint = \"/cas/v1/api-key\"\n",
    "        self.service = \"http://umlsks.nlm.nih.gov\"\n",
    "\n",
    "    def get_tgt(self):\n",
    "        params = {'apikey': self.api_key}\n",
    "        headers = {\"Content-type\": \"application/x-www-form-urlencoded\", \"Accept\": \"text/plain\"}\n",
    "        r = requests.post(self.uri + self.auth_endpoint, data=params, headers=headers)\n",
    "        response = fromstring(r.text)\n",
    "        return response.xpath('//form/@action')[0]\n",
    "\n",
    "    def get_st(self, tgt):\n",
    "        params = {'service': self.service}\n",
    "        headers = {\"Content-type\": \"application/x-www-form-urlencoded\", \"Accept\": \"text/plain\"}\n",
    "        r = requests.post(tgt, data=params, headers=headers)\n",
    "        return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50724ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_semantic_data(term, auth):\n",
    "    \"\"\"Fetch semantic data from UMLS API for a given term.\"\"\"\n",
    "    try:\n",
    "        tgt = auth.get_tgt()\n",
    "        ticket = auth.get_st(tgt)\n",
    "        url = f\"https://uts-ws.nlm.nih.gov/rest/search/current?string={term}&searchType=exact&ticket={ticket}\"\n",
    "        r = requests.get(url)\n",
    "        results = r.json().get(\"result\", {}).get(\"results\", [])\n",
    "        \n",
    "        if results:\n",
    "            semantic_url = f\"https://uts-ws.nlm.nih.gov/rest/content/current/CUI/{results[0]['ui']}?ticket={ticket}\"\n",
    "            details = requests.get(semantic_url).json().get(\"result\", {})\n",
    "            if details:\n",
    "                return {\n",
    "                    \"Term\": term,\n",
    "                    \"Concept_Type\": results[0]['name'],\n",
    "                    \"Semantic_Type\": details.get(\"semanticTypes\", [{}])[0].get(\"name\", \"Unknown\")\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for term {term}: {e}\")\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab25b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_semantics_multithreaded(terms, api_key, max_workers=5):\n",
    "    \"\"\"Group terms by their semantic type using UMLS API with multithreading.\"\"\"\n",
    "    auth = UMLSAuthentication(api_key)\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_term = {executor.submit(fetch_semantic_data, term, auth): term for term in terms}\n",
    "        \n",
    "        for future in as_completed(future_to_term):\n",
    "            term = future_to_term[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if data:\n",
    "                    results.append(data)\n",
    "            except Exception as exc:\n",
    "                print(f\"{term} generated an exception: {exc}\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    path = r\"C:\\Users\\91955\\Desktop\\Data\"\n",
    "    random_file = get_random_file(path)\n",
    "    \n",
    "    if random_file:\n",
    "        with open(random_file, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        processed_words = process_text(cleaned_text)\n",
    "        bigram_list = get_bigrams(processed_words)\n",
    "        \n",
    "        main_words = list(set(processed_words + bigram_list))\n",
    "        noun_words = pos_tagging(main_words)\n",
    "        \n",
    "        api_key = \"your_api_key_here\"\n",
    "        semantic_data = group_by_semantics_multithreaded(noun_words, api_key, max_workers=10)\n",
    "\n",
    "        # Convert to DataFrame and Save\n",
    "        df = pd.DataFrame(semantic_data)\n",
    "        random_filename = f\"complete_{random.randint(1, 20000)}.csv\"\n",
    "        df.to_csv(random_filename, index=False)\n",
    "        print(df.head(60))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
